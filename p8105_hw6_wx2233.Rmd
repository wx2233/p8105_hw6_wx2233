---
title: "p8105_hw6_wx2233"
author: "Weijia Xiong"
date: "11/15/2019"
output: github_document
---
```{r setup, include = FALSE}
library(tidyverse)
library(modelr)
library(mgcv)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
theme_set(theme_minimal() + theme(legend.position = "bottom", text = element_text(size = 15), axis.title.x = element_text(size = 15)))

set.seed(1)
```



## Problem 1

### Load and clean the data
```{r}
birthweight = 
  read.csv("./data/birthweight.csv") %>% 
  janitor::clean_names() %>% 
  drop_na() %>% 
  mutate(
    babysex = recode(babysex,"1" = "male",  "2" = "female") %>% 
      as.factor(),
    frace = recode(frace, "1" = "White", "2" = "Black", "3" = "Asian", "4" = "Puerto Rican", "8" = "Other","9" = "Unknown") %>% 
      as.factor(),
    malform = recode(malform, "0" = "absent", "1" = "present") %>% 
      as.factor(),
    mrace = recode(mrace, "1" = "White", "2" = "Black", "3" = "Asian", "4" = "Puerto Rican", "8" = "Other") %>% 
      as.factor()
  )
    
```



### Propose a regression model

Initial model: 
```{r}
fit1  = lm(bwt ~ babysex + bhead + blength + delwt + fincome + frace + gaweeks + menarche + momage + mheight + mrace + parity + pnumlbw + pnumsga + ppbmi + ppwt + smoken + wtgain, data = birthweight)
summary(fit1)
```
First I built a model using lots of factors, then I remove some inappropriate factors(collinearity or not significant) and add interactions to rebuild the model.

Rebuilt the model:
```{r}
fit2  = lm(bwt ~ babysex * bhead * blength + delwt + gaweeks + ppbmi + smoken, data = birthweight)
summary(fit2)
fit2 %>% 
  broom::tidy() %>% 
  knitr::kable()
```

### Plot of residuals against fitted values
```{r}
birthweight %>% 
  add_residuals(fit2) %>% 
  add_predictions(fit2) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point() +
  labs(
    x = "Fitted Values",
    y = "Residuals",
    title = "Residuals against Fitted Values"
  ) +
  geom_line( y = 0, color = "red")

```



### Compare your model to two others:

One using length at birth and gestational age as predictors (main effects only)

```{r}
fit3  = lm(bwt ~ blength + gaweeks, data = birthweight)
summary(fit3)
fit3 %>% 
  broom::tidy() %>% 
  knitr::kable()
```


One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
Make this comparison in terms of the cross-validated prediction error; use crossv_mc and functions in purrr as appropriate.

```{r}
fit4  = lm(bwt ~ babysex * bhead * blength, data = birthweight)
summary(fit4)
fit4 %>% 
  broom::tidy() %>% 
  knitr::kable()
```

### Cross - Validation
```{r}
cv_df =
  crossv_mc(birthweight, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))

cv_df = 
  cv_df %>% 
  mutate(my_mod = map(train, ~fit2),
         main_mod = map(train, ~fit3),
         interaction_mod = map(train, ~fit4)) %>% 
  mutate(rmse_my = map2_dbl(my_mod, test, ~rmse(model = .x, data = .y)),
         rmse_main    = map2_dbl(main_mod, test, ~rmse(model = .x, data = .y)),
         rmse_interaction = map2_dbl(interaction_mod, test, ~rmse(model = .x, data = .y))
         )

cv_df %>% 
  select(starts_with("rmse")) %>% 
pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + 
  geom_violin(aes(fill = model), alpha = .4)
  
```


From the plot we could see that the rmse of main model is the highest. And my model's rmse is lowest. It might indicates that my model fits better.


## Problem 2
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

The boostrap is helpful when you’d like to perform inference for a parameter / value / summary that doesn’t have an easy-to-write-down distribution in the usual repeated sampling framework. We’ll focus on a simple linear regression with tmax as the response and tmin as the predictor, and are interested in the distribution of two quantities estimated from these data:

r̂ 2
log(β̂ 0∗β̂ 1)

Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of these two quantities. Plot the distribution of your estimates, and describe these in words. Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r̂ 2 and log(β̂ 0∗β̂ 1). Note: broom::glance() is helpful for extracting r̂ 2 from a fitted regression, and broom::tidy() (with some additional wrangling) should help in computing log(β̂ 0∗β̂ 1).

### Bootstrap
```{r}
estimate_fun = function(mod){
  mod_results =
    mod %>% 
    broom::tidy()
  tibble(
    r_squared = pull(mod %>% broom::glance(),adj.r.squared),
    log_result = log(pull(mod_results,estimate)[1] * pull(mod_results,estimate)[2]),
  )
}

weather_results = 
 weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~ lm(tmax ~ tmin, data = .x)),
    results = map(models, estimate_fun)) %>% 
  unnest(results)

weather_results %>% 
  head(5)
```

### Distribution of two estimate quantities
```{r}
mean_and_sd = function(x) {
  estimate_mean = mean(x)
  estimate_sd = sd(x)
  tibble(
    estimate_mean,
    estimate_sd
  )
}

estimate_results = 
weather_results %>% 
  select(-strap,-.id,-models) %>% 
  map(~mean_and_sd(.)) %>% bind_rows() %>% 
  mutate(
    quantity = c("r_squared","logbeta01")
  ) %>% 
  select(quantity,everything())

estimate_results %>% 
  knitr::kable()
```

Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of these two quantities. Plot the distribution of your estimates, and describe these in words. 

The mean of r square is 0.91, the sd of r square is 0.0085, which shows that the r square is high, the model is explainable.
The mean of log(beta0beta1) is 2.01, the sd of log(beta0beta1) is 0.024.


The r_squared is right skewed.
```{r}
weather_results %>% 
  ggplot(aes(x = r_squared, y = ..density..)) +
  geom_histogram(binwidth = 0.001, position = "dodge",fill = "#FF9999") +
  geom_density(alpha = .4, adjust = .5, color = "#FF9999",fill = "#FF9999")
```

The log(beta0beta1)  is almost normal distributed.

```{r}

weather_results %>% 
  ggplot(aes(x = log_result, y = ..density..)) +
  geom_histogram(binwidth = 0.002, position = "dodge",fill = "#99CCFF") +
  geom_density(alpha = .4, adjust = .5, color = "#99CCFF",fill = "#99CCFF") +
  labs(
    x = "log(beta0*beta1)"
  )
```


Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r̂ 2 and log(β̂ 0∗β̂ 1). 


a 95% confidence interval: mean ± 1.64 × sd/sqrt(n) 

```{r}

estimate_results %>% 
  mutate(
    CI_lwr =  estimate_mean - 1.64 * estimate_sd/sqrt(5000),
    CI_upr =  estimate_mean + 1.64 * estimate_sd/sqrt(5000)
  ) %>% 
  knitr::kable()
```

